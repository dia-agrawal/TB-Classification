{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, time, random, glob, os\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Audio, clear_output\n",
    "#from pyAudioAnalysis import audioBasicIO, audioFeatureExtraction\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal as sig\n",
    "from pydub import AudioSegment\n",
    "from pydub import effects\n",
    "import sklearn\n",
    "from sklearn import neighbors, datasets, metrics, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from numpy.random.mtrand import permutation\n",
    "import random\n",
    "import pandas as pd \n",
    "import librosa\n",
    "import re\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "from silero_vad import (load_silero_vad,\n",
    "                    read_audio,\n",
    "                    get_speech_timestamps,\n",
    "                    save_audio,\n",
    "                    VADIterator,\n",
    "                    collect_chunks)\n",
    "\n",
    "silero_model = load_silero_vad(onnx=False)\n",
    "\n",
    "def use_silero_vad(file_path, Fs=16000, retun_seconds=True):\n",
    "    wav = read_audio(file_path, sampling_rate=16000)\n",
    "    speech_timestamps = get_speech_timestamps(wav, silero_model, threshold=0.5, sampling_rate=Fs, return_seconds=retun_seconds)\n",
    "    # print(speech_timestamps)\n",
    "\n",
    "    return speech_timestamps\n",
    "\n",
    "\n",
    "def add_duration(duration_list, window_size, time_in_seconds=True):\n",
    "    total_duration = 0 \n",
    "    unpaked_duration = [] \n",
    "\n",
    "    for data in duration_list:\n",
    "        if isinstance(data, dict):\n",
    "            start = data['start']\n",
    "            end   = data['end']\n",
    "\n",
    "            total_duration += end-start\n",
    "            unpaked_duration.append([start, end])\n",
    "\n",
    "        elif isinstance(data, AudioSegment):\n",
    "            total_duration += data.duration_seconds\n",
    "            print(data.duration_seconds)\n",
    "        else:\n",
    "            assert TypeError, f'Type: {type(data)}'\n",
    "            break\n",
    "\n",
    "        if time_in_seconds==False: # it's in ms convert to s \n",
    "            total_duration /= 1000 \n",
    "\n",
    "    return total_duration, unpaked_duration\n",
    "\n",
    "def get_window_timestamp_wo_df(file_duration, window_size=4, num_window=10, min_step=0.5, debug=False):\n",
    "    step_size=(file_duration-window_size)/(num_window - 1) \n",
    "    valid_clips = []\n",
    "    start = 0\n",
    "    end = window_size\n",
    "    for i in range(num_window):\n",
    "        if start + window_size > file_duration: #, f\"{i} {start} {window_size} {file_duration}\"\n",
    "            start = file_duration - window_size\n",
    "        valid_clips.append(int(start * 10 ** 3) / 10 ** 3)\n",
    "        start += step_size\n",
    "    assert len(valid_clips) == num_window, f\"Expected {num_window} valid clips, but got {len(valid_clips)}\"\n",
    "    \n",
    "    return valid_clips\n",
    "\n",
    "\n",
    "def get_window_timestamp(defect_list, file_duration, window_size=4, num_window=10, min_step=0.1, debug=False):\n",
    "    \n",
    "    windows = []\n",
    "    MaxPossible = []\n",
    "    valid_clips = []\n",
    "    remaining = num_window \n",
    "\n",
    "    defect_list = np.array(defect_list)\n",
    "\n",
    "    if len(defect_list) == 0 : \n",
    "        return get_window_timestamp_wo_df(file_duration, window_size, num_window, min_step, debug)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(defect_list[:, 0]) - 1:\n",
    "        defect_list[i, 1] = defect_list[i + 1, 1] if defect_list[i + 1, 0] <= defect_list[i, 1] else defect_list[i, 1]\n",
    "        defect_list = np.delete(defect_list, i + 1, 0) if defect_list[i + 1, 0] <= defect_list[i, 1] else defect_list\n",
    "        if i+1 < len(defect_list) and defect_list[i + 1, 0] > defect_list[i, 1]:\n",
    "            i += 1 \n",
    "        else:\n",
    "            i = 0\n",
    "\n",
    "\n",
    "    for i in range (len(defect_list[:,0])):\n",
    "        start = 0 if  i == 0 else defect_list[i-1,1]\n",
    "        windows.extend([defect_list[i,0]-start, start, defect_list[i,0]])\n",
    "        windows.extend([file_duration-defect_list[-1,1],  defect_list[-1,1], file_duration]) if i  == len(defect_list[:,0])-1 else []\n",
    "    windows = np.reshape(np.array(windows), (-1,3))\n",
    "    windows = windows[(windows)[:,0].argsort()]\n",
    "\n",
    "    new = np.squeeze(windows[np.argwhere(windows[:,0]>=window_size)],  axis=1)\n",
    "    if debug:\n",
    "        print('window:', windows)\n",
    "        print('new window: ', new)\n",
    "\n",
    "    MaxPossible = (((new[:,0] - window_size) / min_step + 1)).flatten()\n",
    "    \n",
    "    # print(np.sum(MaxPossible) , num_window)\n",
    "    if np.sum(MaxPossible) < num_window : \n",
    "        # skip the file\n",
    "        print(f'Possible windows: {np.sum(MaxPossible)} less than expected windows: {num_window}')\n",
    "        return None\n",
    "\n",
    "    proportional = [num_window*i/(np.sum(MaxPossible)) for i in MaxPossible]\n",
    "    #validclips \n",
    "    assert  len(new[:,0]) == len(MaxPossible) == len(proportional)\n",
    "\n",
    "    for x in range(len(new[:,0])):\n",
    "        w = int(proportional[x] + 1) if int(proportional[x] + 1) < remaining else remaining \n",
    "        remaining -= w \n",
    "        new_step = (new[x,0]-window_size)/ (w - 1) if w > 1 else 0  # Avoid division by zero\n",
    "        start = new[x,1]\n",
    "        for i in range(w):\n",
    "            valid_clips.append(int(start * 10 ** 3) / 10 ** 3)\n",
    "            # valid_clips.append(round(start, 3))\n",
    "            start = new[x,2] - window_size if i == w - 1 else start + new_step\n",
    "    assert  len(valid_clips) == num_window , f\"Expected {num_window} valid clips, but got {len(valid_clips)}\"\n",
    "    \n",
    "\n",
    "    return valid_clips\n",
    "\n",
    "\n",
    "\n",
    "def get_objList(file, win_timestamp,  window_size=4, Fs=16000):\n",
    "\n",
    "    dataObj = AudioSegment.from_file(file)\n",
    "    dataObj  = dataObj.set_frame_rate(Fs)\n",
    "\n",
    "    objList = []\n",
    "    for ts in win_timestamp:\n",
    "        start = round(ts * 1000) \n",
    "        end = start + (window_size * 1000)\n",
    "\n",
    "        # print(ts, start, end)\n",
    "\n",
    "        obj = dataObj[start:end]\n",
    "        objList.append(obj)\n",
    "\n",
    "    return objList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# NOTE Run this cell if you have labels in a csv file - (UCSF R2D2_validation data)\n",
    "##################\n",
    "import torch\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from utils.pre_generator_noderiv import get_click_objects\n",
    "from utils.pre_generator_noderiv import create_dirs\n",
    "from utils.refactored_common import gen_mel_feature, yaml_load\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "DEBUG=False\n",
    "param = yaml_load('./old_files/gen_config.yaml')\n",
    "\n",
    "tmp_path = './data/new_R2D2_Data/R2D2_Train_Data'   # audium base\n",
    "\n",
    "csv_file = os.path.join('./data/new_R2D2_Data/R2D2 lung sounds metadata_TRAIN_2025.05.08_v3.csv')\n",
    "patient_data=pd.read_csv(csv_file)\n",
    "patient_data.head()\n",
    "\n",
    "normal_patient_ids = patient_data[patient_data['Sputumxpertreferencestandard'] == \"TB Negative\"]['StudyID'].tolist()\n",
    "tb_patient_ids = patient_data[patient_data['Sputumxpertreferencestandard'] == \"TB Positive\"]['StudyID'].tolist()\n",
    "patient_country = {p:c for p, c in zip(patient_data['StudyID'], patient_data['Country'])}\n",
    "\n",
    "print('normal patients:', len(normal_patient_ids), 'tb patients:',len(tb_patient_ids))\n",
    "\n",
    "#  (60% + 10% + 10% + 20% \n",
    "# 50% + + 10% + 10% + 30% test\n",
    "# (55% + 10% + 10% + 25% split\n",
    "# data_split = [0.3, 0.4, 0.5]\n",
    "# (60% + 10% + 10% + 20% test)\n",
    "# 50%+ 10% + 10% + 30%)  + ignore (100% + 0% + 0%) + 150_new_patient (70% + 10% + 10% + 10%)\n",
    "# (30%+ 10% + 10% + 50%)\n",
    "# (55% + 15% + 10% + 20%)  \n",
    "# 0:55, 55:70, 70:80, 80:100\n",
    "# train: 0.65, val_train: 0.70, val_test: 0.7-0.8 test: 0.80-1.0\n",
    "# train: 0.65, (val_train: 0.70, val_test: 0.7-0.8) test: 0.80-1.0\n",
    "# data_split = [0.80, 0.81, 0.90]\n",
    "\n",
    "test_mode = True\n",
    "train_test_normal = []\n",
    "train_test_tb = []\n",
    "train_normal = []\n",
    "train_tb = []\n",
    "patient_id_pattern = r'^R2D2\\d{5}$'\n",
    "\n",
    "Fs = 16000\n",
    "expected_files = 20\n",
    "min_expected_files = 16\n",
    "expected_duration = 16\n",
    "duration_thresh = 4\n",
    "defect_dict = {} # speech [{start, end}], peaks [{start, end}]\n",
    "filtered_file_dict = {}\n",
    "DEBUG = False\n",
    "repeat_files = True ## option 1 \n",
    "window_size = 4 \n",
    "num_window = 9\n",
    "min_step = 0.5\n",
    "\n",
    "# features to gen \n",
    "gen_mel = False\n",
    "gen_stft = True\n",
    "\n",
    "total_patients_found = [] # just for debug\n",
    "pateint_label_notFound = []\n",
    "\n",
    "# is good == 0 -> use for train only \n",
    "# is good == 1 -> ok use in train and val\n",
    "# is good == 2 -> empty\n",
    "for subdir, dirs, files in os.walk(tmp_path):\n",
    "    # print (subdir, len(files)) #, files)\n",
    "    is_good = 1\n",
    "    tmp_files = []\n",
    "    dirname = os.path.basename(subdir)\n",
    "    tmp_files.extend(glob.glob(os.path.join(subdir, \"*.wav\")))\n",
    "    print('subdir', subdir, len(tmp_files))\n",
    "    if len(tmp_files) == 0:\n",
    "        is_good = 2\n",
    "        continue \n",
    "    \n",
    "    # continue\n",
    "    # if (dirname != 'R2D204281'): continue \n",
    "    # print(subdir)\n",
    "    # if ('R2D204227' not in subdir): continue        \n",
    "    \n",
    "    filtered_files = []\n",
    "    # tmp_files = []\n",
    "    for file in tmp_files:\n",
    "        try :\n",
    "            filename = os.path.basename(file)\n",
    "            dataObj = AudioSegment.from_file(file)\n",
    "            assert dataObj.frame_rate == Fs\n",
    "        except:\n",
    "            print (\"File read error:\", filename)\n",
    "            continue\n",
    "\n",
    "        if dataObj.duration_seconds < expected_duration or dataObj.duration_seconds > 21:\n",
    "            print(f'Duration error, file: {filename}, duration: {dataObj.duration_seconds}')\n",
    "\n",
    "        else: ## good file > proceed checks\n",
    "            speech_duration = use_silero_vad(file, Fs)\n",
    "            if DEBUG: print('speech', speech_duration)\n",
    "            total_speech_duration, speech_duration_ = add_duration(speech_duration, window_size)\n",
    "            if DEBUG: print(f'File : {filename}, speech duration: {total_speech_duration}s')\n",
    "\n",
    "            if total_speech_duration > duration_thresh:\n",
    "                print(f'speech duration is more skipping file: {file}, duration: {total_speech_duration}')\n",
    "                continue \n",
    "\n",
    "            # # click_objList = get_click_objects([file], duration=100, Fs=Fs, disable_tqdm=True)\n",
    "            # click_objList = get_click_objects([file], duration=0.1, Fs=Fs, dB_threshold=-5, disable_tqdm=True, DEBUG=False)\n",
    "            # # click_objList[0][2] = []\n",
    "            # click_duration_list = click_objList[0][2] if len(click_objList[0])>=3 else click_objList[0][1]\n",
    "            # print('click', click_duration_list)        \n",
    "            # total_peak_duration, unpaked_peak_duraiton = add_duration(click_duration_list, window_size)\n",
    "            # if DEBUG:print(f'File : {filename}, peak duration: {total_peak_duration}s, unpaked peak_duration: ', unpaked_peak_duraiton)\n",
    "            # if total_peak_duration > duration_thresh: \n",
    "            #     print(f'peak duration is more skipping file: {file}')\n",
    "            #     continue\n",
    "\n",
    "            # if DEBUG: print(f'File : {filename}, total duration: {(total_speech_duration + total_peak_duration)}s')\n",
    "            # if (total_speech_duration + total_peak_duration) > duration_thresh:\n",
    "            #     print(f'defect duration is more skipping file: {file}')\n",
    "            #     continue\n",
    "            \n",
    "            defect_ = speech_duration_ #+ unpaked_peak_duraiton\n",
    "            win_timestamp = get_window_timestamp(defect_, dataObj.duration_seconds, window_size, num_window, min_step, debug=DEBUG)\n",
    "            if DEBUG: print('win_timestamp', file, win_timestamp)\n",
    "\n",
    "            if win_timestamp != None and len(win_timestamp) == num_window:\n",
    "                defect_dict[filename] = win_timestamp\n",
    "            else:\n",
    "                print(f'File: {filename}, expected win: {num_window}, windows got: {win_timestamp}')\n",
    "                continue \n",
    " \n",
    "            filtered_files.append(file)\n",
    "                \n",
    "    if len(filtered_files) > expected_files:\n",
    "        filtered_files = filtered_files[0:expected_files]\n",
    "    elif min_expected_files <= len(filtered_files) < expected_files:  ## option 1 \n",
    "        # is_good = 0  # FIXME gowtham \n",
    "        # print (\"num files issue 2\", subdir, len(filtered_files))\n",
    "        if repeat_files:\n",
    "            if DEBUG: print('???', len(filtered_files))\n",
    "            filtered_files_ = []\n",
    "            filtered_files_.extend(filtered_files)\n",
    "            files_need = expected_files - len(filtered_files)\n",
    "            filtered_files_.extend(random.choices(filtered_files, k=files_need))\n",
    "            filtered_files = filtered_files_\n",
    "\n",
    "    else:\n",
    "        print(f'ERROR: file count criteria not matching. filtered files: {len(filtered_files)}')\n",
    "    #     continue\n",
    "    # break\n",
    "    print('subdir3', subdir)\n",
    "    subdir_parts = subdir.split('/')\n",
    "    patient_id = None\n",
    "    for part in subdir_parts[::-1]:\n",
    "        if re.match(patient_id_pattern, part):\n",
    "            patient_id = part\n",
    "            # assert patient_id not in total_patients_found, f'{patient_id}, {total_patients_found}'\n",
    "\n",
    "            if patient_id not in total_patients_found and (patient_id in normal_patient_ids or patient_id in tb_patient_ids):\n",
    "                total_patients_found.append([patient_id, len(tmp_files)])\n",
    "            break\n",
    "    else:\n",
    "        print('Error in patient id finding ', subdir_parts, subdir)\n",
    "    assert patient_id is not None, f'{patient_id}'\n",
    "    \n",
    "    if patient_id not in filtered_file_dict:\n",
    "        filtered_file_dict[patient_id] = filtered_files\n",
    "    else:\n",
    "        if DEBUG: print('adding into existing patient data')\n",
    "        filtered_file_dict[patient_id].extend(filtered_files)\n",
    "\n",
    "    if DEBUG: print('!!!!', patient_id, len(filtered_files))\n",
    "\n",
    "    if len(filtered_files) == 0 or patient_id is None:\n",
    "        pass \n",
    "    elif len(filtered_files) == expected_files:\n",
    "        if patient_id in normal_patient_ids:\n",
    "            train_test_normal.append(patient_id)\n",
    "        elif patient_id in tb_patient_ids :\n",
    "            train_test_tb.append(patient_id)\n",
    "        else:\n",
    "            print('label not found for pid', patient_id)\n",
    "            pateint_label_notFound.append(patient_id)\n",
    "    else:\n",
    "        if patient_id in normal_patient_ids:\n",
    "            train_normal.append(patient_id)\n",
    "        elif patient_id in tb_patient_ids :\n",
    "            train_tb.append(patient_id)\n",
    "        else:\n",
    "            print('label not found for pid', patient_id)\n",
    "            pateint_label_notFound.append(patient_id)\n",
    "\n",
    "\n",
    "print (f'train test normal: {len(train_test_normal)}, train normal: {len(train_normal)}, train test tb: {len(train_test_tb)}, train tb: {len(train_tb)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"./data/dia_split2_2/\"\n",
    "\n",
    "data_split = [0.50, 0.60, 0.70]\n",
    "\n",
    "\n",
    "if test_mode: \n",
    "    print (\"Number used only for training\", len(train_normal), len(train_tb))\n",
    "    \n",
    "    # train_split_normal = []\n",
    "    # val_split_normal = []\n",
    "    # test_split_normal = train_test_normal\n",
    "\n",
    "    # train_split_tb = []\n",
    "    # val_split_tb = []\n",
    "    # test_split_tb = train_test_tb\n",
    "    \n",
    "    # train_normal = []\n",
    "    # train_tb = []\n",
    "\n",
    "    random.shuffle(train_test_normal)\n",
    "    random.shuffle(train_test_tb)\n",
    "    l1 = len(train_test_normal)\n",
    "\n",
    "    train_split_normal = train_test_normal[0:int(data_split[0]*l1)]\n",
    "    val_train_split_normal = train_test_normal[int(data_split[0]*l1): int(data_split[1]*l1)]\n",
    "    val_test_split_normal = train_test_normal[int(data_split[1]*l1): int(data_split[2]*l1)]\n",
    "    test_split_normal = train_test_normal[int(data_split[2]*l1): ]\n",
    "\n",
    "    l1 = len(train_test_tb)\n",
    "    train_split_tb = train_test_tb[0:int(data_split[0]*l1)]\n",
    "    val_train_split_tb = train_test_tb[int(data_split[0]*l1): int(data_split[1]*l1)]\n",
    "    val_test_split_tb = train_test_tb[int(data_split[1]*l1): int(data_split[2]*l1)]\n",
    "    test_split_tb = train_test_tb[int(data_split[2]*l1): ]\n",
    "\n",
    "\n",
    "else :\n",
    "    assert False\n",
    "    # gentype split \n",
    "    random.shuffle(train_test_normal)\n",
    "    random.shuffle(train_test_tb)\n",
    "    l1 = len(train_test_normal)\n",
    "\n",
    "    train_split_normal = train_test_normal[0:int(data_split[0]*l1)]\n",
    "    val_split_normal = train_test_normal[int(data_split[0]*l1): int(data_split[1]*l1)]\n",
    "    test_split_normal = train_test_normal[int(data_split[1]*l1): ]\n",
    "\n",
    "    l1 = len(train_test_tb)\n",
    "    train_split_tb = train_test_tb[0:int(data_split[0]*l1)]\n",
    "    val_split_tb = train_test_tb[int(data_split[0]*l1): int(data_split[1]*l1)]\n",
    "    test_split_tb = train_test_tb[int(data_split[1]*l1): ]\n",
    "\n",
    "    #  [0.0, 1.537, 3.075, 7.175, 8.645, 10.116, 11.587, 13.058, 14.529, 15.999]\n",
    "print(train_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(total_patients_found)\n",
    "_, idx = np.unique(p[:, 0], return_index=True)\n",
    "flattern = p[np.sort(idx)]\n",
    "print(flattern.shape)\n",
    "\n",
    "p = flattern[::]\n",
    "\n",
    "print(p.shape)\n",
    "p = np.array(p[p[:, 1] != 0])\n",
    "print(p.shape)\n",
    "\n",
    "pid = p[:, :1].flatten().tolist()\n",
    "files = p[:, 1:].flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('total patients', len(set(total_patients_found)))\n",
    "print('total test normal :', len(train_test_normal))\n",
    "print('total test tb     :', len(train_test_tb))\n",
    "print('data split   :', data_split)\n",
    "\n",
    "print('train normal :', len(train_normal))\n",
    "print('train tb     :', len(train_tb))\n",
    "\n",
    "print(f'total patients: {len(train_test_normal) + len(train_test_tb) + len(train_normal) + len(train_tb)}')\n",
    "\n",
    "print(len(train_split_normal), len(val_train_split_normal), len(val_test_split_normal), len(test_split_normal))\n",
    "print(len(train_split_tb), len(val_train_split_tb), len(val_test_split_tb), len(test_split_tb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# NOTE Run this cell if you have labels in a csv file - (UCSF R2D2_validation data)\n",
    "##################\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# target_dBFS = -28\n",
    "Fs = 8000\n",
    "n_fft = 512      # 512 -> 0.4ms, 8000 -> 64 or 128\n",
    "hop_length = 160 #160 #512 #1200  # 160 # [ 240 -> for impulse ]\n",
    "win_length = 512\n",
    "window = 'hann'\n",
    "n_mels = 128\n",
    "    \n",
    "def process_file(f, duration=20) :\n",
    "    if isinstance(f, str):\n",
    "        dataObj = AudioSegment.from_file(f)\n",
    "    elif isinstance(f, AudioSegment):\n",
    "        dataObj = f\n",
    "    else:\n",
    "        assert False, f'Unknown data type {type(f)}'            \n",
    "    \n",
    "    # FIXME: Change the sampling rate -- check frame rate API in pydub\n",
    "    dataObj = dataObj.set_frame_rate(Fs)\n",
    "    \n",
    "    # print(dataObj.duration_seconds, duration)\n",
    "    if dataObj.duration_seconds != duration :\n",
    "        print (\"Duration issue\", f, dataObj.duration_seconds)\n",
    "        return None\n",
    "\n",
    "    # change_in_dBFS = target_dBFS - dataObj.dBFS\n",
    "    # dataObj = dataObj.apply_gain(change_in_dBFS)\n",
    "    data = dataObj.get_array_of_samples()\n",
    "    data = np.array(data)/32768\n",
    "    # print('data:', data.shape, data[0])\n",
    "\n",
    "    if False:\n",
    "        t = gen_mel_feature_local(data, Fs, window)\n",
    "        print (t.shape)\n",
    "        return  t\n",
    "    else:\n",
    "        return gen_mel_feature(param, data, Fs, n_fft, hop_length, win_length, n_mels, window=window, enable_librosa=True, gen_stft=True) # librosa = True\n",
    "\n",
    "\n",
    "# tuples_ = [\n",
    "#         #    (train_split_normal, \"train\", \"good\"), \n",
    "#         #    (train_normal, \"train\", \"good\"), \n",
    "#         #    (test_split_normal, \"test\", \"good\"), \n",
    "#         #    (val_split_normal, \"val\", \"good\"), \n",
    "#         #    (train_split_tb, \"train\", \"bad\"), \n",
    "#         #    (train_tb, \"train\", \"bad\"), \n",
    "#         #    (test_split_tb, \"test\", \"bad\"), \n",
    "#         #    (val_split_tb, \"val\", \"bad\")  \n",
    "#             (train_test_normal, \"test\", \"good\"), \n",
    "#             (train_test_tb, \"val\", \"bad\"), \n",
    "#            ]\n",
    "\n",
    "# if set(train_split_normal).issuperset(set(train_normal)) == False:\n",
    "#     train_split_normal.extend(train_normal)\n",
    "# if set(train_split_tb).issuperset(set(train_tb)) == False:\n",
    "#     train_split_tb.extend(train_tb)\n",
    "\n",
    "train_split_normal.extend(train_normal)\n",
    "train_split_tb.extend(train_tb)\n",
    "\n",
    "print('normal', len(train_split_normal))\n",
    "print('tb', len(train_split_tb))\n",
    "\n",
    "val_split_normal = val_train_split_normal.copy()\n",
    "val_split_normal.extend(val_test_split_normal)\n",
    "val_split_tb = val_train_split_tb.copy()\n",
    "val_split_tb.extend(val_test_split_tb)\n",
    "\n",
    "tuples_ = [[train_split_normal[:int(len(train_split_normal)*1)], test_split_normal[:int(len(test_split_normal)*1)], val_split_normal[:int(len(val_split_normal)*1)]],\n",
    "           [train_split_tb, test_split_tb, val_split_tb]]\n",
    "\n",
    "gen_types = ['train', 'test', 'val']\n",
    "classList = ['good', 'bad']\n",
    "\n",
    "pid_seen = []\n",
    "for idx, data_path_list in enumerate(tuples_):\n",
    "    #print('!!!', idx, len(data_path_list))\n",
    "    print(data_path_list)\n",
    "    for genId, gen_path_ist in enumerate(data_path_list):\n",
    "        genType = gen_types[genId]\n",
    "        classname = classList[idx]\n",
    "        print(outdir, genType, classname)\n",
    "        # break\n",
    "        # create_dirs(outdir, classname)\n",
    "        #     for t in [\"train\", \"val_train\", \"val_test\", \"test\"] :\n",
    "        path_ = os.path.join(outdir, genType, classname)\n",
    "        if not os.path.exists(path_) :\n",
    "            os.makedirs(path_)\n",
    "\n",
    "        \n",
    "        print(genId, gen_types[genId], len(gen_path_ist))\n",
    "        for pid in tqdm(gen_path_ist):\n",
    "            # files = glob.glob(os.path.join(path, '*.wav'))\n",
    "            # dirname = os.path.basename(path)\n",
    "            files = filtered_file_dict.get(pid, [])\n",
    "            # print(files)\n",
    "            #print('!!!', len(files))\n",
    "            pid_seen.append(pid)\n",
    "            for file in files:\n",
    "                filename = os.path.basename(file)\n",
    "                file_win_ts = defect_dict.get(filename, [])\n",
    "                file_win_ts.sort()\n",
    "                win_objlist = get_objList(file, file_win_ts, window_size, Fs)\n",
    "                # print(len(win_objlist))\n",
    "\n",
    "                for i, obj in enumerate(win_objlist):\n",
    "                    x = file.split('/')\n",
    "                    # country = '' #x[-4]\n",
    "                    # pid = None\n",
    "                    # for part in x[::-1]:\n",
    "                    #     if re.match(patient_id_pattern, part):\n",
    "                    #         pid = part\n",
    "                    #         break\n",
    "                    # if pid is None:\n",
    "                    #     print('Error not able fo find patient id', x)\n",
    "                    #     continue\n",
    "\n",
    "                    country = patient_country[pid]\n",
    "                    # assert country is None, f'country not found for {pid}'\n",
    "\n",
    "                    # add start ts in filename \n",
    "                    ts = file_win_ts[i]\n",
    "                    baseName =  pid + \"_\" + os.path.splitext(os.path.basename(file))[0] + f'_{i}' #_{ts}'\n",
    "                    #dst = os.path.join(outdir, genType, classname, baseName + '.flac')\n",
    "                    mel_dst = os.path.join(outdir, genType, classname, \"_\" + baseName + '.pt')\n",
    "                    # FIXME: Create pt and change path\n",
    "\n",
    "                    # print(dst, mel_dst)\n",
    "                    #obj.export(dst, format=\"flac\")\n",
    "                    print(mel_dst)\n",
    "                    #print(mel_feature)\n",
    "                    mel_feature = process_file(obj, duration=window_size)\n",
    "                    #print(mel_dst, mel_feature.shape, mel_feature[0][0])\n",
    "\n",
    "                    # print(mel_dst)\n",
    "                    # with open(mel_dst, 'wb+') as fh:\n",
    "                    #     np.savez_compressed(fh, f=mel_feature)\n",
    "\n",
    "                    # FIXME: torch.save()       \n",
    "                    torch.save(torch.tensor(mel_feature), mel_dst)\n",
    "\n",
    "    # break\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(pid_seen)), len(pid_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_seen2 = []\n",
    "for idx, data_path_list in enumerate(tuples_):\n",
    "    #print('!!!', idx, len(data_path_list))\n",
    "    for genId, gen_path_ist in enumerate(data_path_list):\n",
    "        genType = gen_types[genId]\n",
    "        classname = classList[idx]\n",
    "\n",
    "        # create_dirs(outdir, classname)\n",
    "\n",
    "        print(genId, gen_types[genId], len(gen_path_ist))\n",
    "        for pid in tqdm(gen_path_ist):\n",
    "            # files = glob.glob(os.path.join(path, '*.wav'))\n",
    "            # dirname = os.path.basename(path)\n",
    "            files = filtered_file_dict.get(pid, [])\n",
    "            # print(files)\n",
    "            #print('!!!', len(files))\n",
    "            pid_seen2.append(pid)\n",
    "\n",
    "len(pid_seen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in filtered_file_dict:\n",
    "    if len(filtered_file_dict[pid]) == 0:\n",
    "        print(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
